{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load FAQs Dataset"
      ],
      "metadata": {
        "id": "RjpjF34PSOeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "Questions_PATH=\"./\"\n",
        "\n",
        "def load_dataset(dataset_path=Questions_PATH):\n",
        "    csv_path = os.path.join(dataset_path, \"faqs.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "data_original = load_dataset()\n",
        "data = load_dataset()"
      ],
      "metadata": {
        "id": "OhJtinNxSNAG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing - Input"
      ],
      "metadata": {
        "id": "W_mUV2KnSh7X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzUPlpWmbfBx",
        "outputId": "b79c391b-41e6-43e5-b7de-f48528548af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "data['stemmed_question'] = ''\n",
        "\n",
        "# Define stemmer\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "# Define stopwords\n",
        "stopwords = set(stopwords.words('arabic') + list(punctuation))\n",
        "\n",
        "# Prepare data with stemming and remove stopwords\n",
        "def preprocess(text):\n",
        "    words = word_tokenize(str(text))\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    data['stemmed_question'][index] = preprocess(row['question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing - Output"
      ],
      "metadata": {
        "id": "SpgnFHoMTiL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "label_cat = data[['answer']]\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "label_cat_1hot = cat_encoder.fit_transform(label_cat).toarray()\n",
        "label_cat_1hot"
      ],
      "metadata": {
        "id": "OGZ3-AmmTlCZ",
        "outputId": "d2f01853-0fd5-4eee-b46d-9c2964bcae5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle and Split into train and test sets"
      ],
      "metadata": {
        "id": "6yoy8cGDT31P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]\n",
        "\n",
        "sentences = data['stemmed_question'].to_numpy()\n",
        "X, Y = unison_shuffled_copies(sentences, label_cat_1hot)\n",
        "\n",
        "training_size = int(len(X) * 0.8)\n",
        "train_X, test_X = X[:training_size], X[training_size:]\n",
        "train_Y, test_Y = Y[:training_size], Y[training_size:]\n",
        "\n",
        "print(len(train_X))\n",
        "print(len(train_Y))\n",
        "\n",
        "print(len(test_X))\n",
        "print(len(test_Y))"
      ],
      "metadata": {
        "id": "NeOjBZOiT8gK",
        "outputId": "c0b0cf3b-c209-4a74-9dce-0dd7af077f79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "8\n",
            "3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequening and Padding"
      ],
      "metadata": {
        "id": "dajRisnfUNte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define tokenizer\n",
        "oov_tok = \"<OOV>\"\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "\n",
        "# Fit tokenizer on data\n",
        "tokenizer.fit_on_texts(X)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert data to sequences\n",
        "train_X_seq = tokenizer.texts_to_sequences(train_X)\n",
        "test_X_seq = tokenizer.texts_to_sequences(test_X)\n",
        "\n",
        "# Pad sequences\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "max_length = 100   #max([len(seq) for seq in sequences])\n",
        "\n",
        "train_X_pad = pad_sequences(train_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "test_X_pad = pad_sequences(test_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(test_X)\n",
        "print(test_X_seq)\n",
        "print(train_X)\n",
        "print(test_X_pad)"
      ],
      "metadata": {
        "id": "7AZXJYC_Uz1P",
        "outputId": "9ce9878f-95cb-4fd8-c779-c1b7a5bda6e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['قيم قبل الي شرك نصة' 'الب سوق توفر نصة'\n",
            " 'يمكن سجل كثر حسب نفس شخص علي نصة']\n",
            "[[20, 21, 22, 23, 2], [24, 25, 26, 2], [3, 6, 27, 7, 28, 29, 4, 2]]\n",
            "['يمك طلع علي عمل سبق' 'نصة' 'يمكن سجل ورد علي نصة' 'يمكن طلع فصل حسب'\n",
            " 'علن' 'خبر' 'يمكن سجل حسب بدن رقم هتف' 'يمكن طلع علي فصل وثق طبع']\n",
            "[[20 21 22 23  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [24 25 26  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]\n",
            " [ 3  6 27  7 28 29  4  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Deep Learning Model"
      ],
      "metadata": {
        "id": "q_16v7e7WTaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "out_dim = 64\n",
        "lstm_dim = 64\n",
        "dense_dim = 64\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=100, output_dim=out_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(11, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "opt = Adamax(learning_rate=0.03, beta_1=0.8, beta_2=0.9999)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RaotmlbRA2Sb",
        "outputId": "cf59b73d-b109-44b0-b426-1e93ce15bbf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 100, 64)           6400      \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 128)              66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 11)                715       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 81,419\n",
            "Trainable params: 81,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Validate Model"
      ],
      "metadata": {
        "id": "gneKCounWdI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "# Train model\n",
        "model.fit(train_X_pad, train_Y, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgwkLutolrmw",
        "outputId": "3cc54300-ed83-4752-8492-e4414d240b32"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3957 - accuracy: 0.1250\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 2.2375 - accuracy: 0.1250\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 2.3191 - accuracy: 0.3750\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 2.1133 - accuracy: 0.2500\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 1.8162 - accuracy: 0.3750\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 1.3875 - accuracy: 0.6250\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.1093 - accuracy: 0.6250\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.2654 - accuracy: 0.3750\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.6414 - accuracy: 0.8750\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4883 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.3702 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1867 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.1082 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0603 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0154 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 9.8576e-04 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 7.8851e-04 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 6.5627e-04 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 5.6525e-04 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 5.0097e-04 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 4.5305e-04 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 4.1645e-04 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 3.8790e-04 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 3.6467e-04 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 3.4547e-04 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 3.2918e-04 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 3.1528e-04 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 3.0327e-04 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 2.9261e-04 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 2.8322e-04 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 2.7478e-04 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 2.6715e-04 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.6022e-04 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 2.5385e-04 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.4802e-04 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 2.4262e-04 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 2.3760e-04 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.3289e-04 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 2.2849e-04 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.2435e-04 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 2.2041e-04 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 2.1665e-04 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 2.1312e-04 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 2.0974e-04 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 2.0654e-04 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 2.0354e-04 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 2.0052e-04 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.9763e-04 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 1.9490e-04 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.9222e-04 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.8972e-04 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 1.8720e-04 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 1.8477e-04 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.8248e-04 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 1.8020e-04 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 1.7796e-04 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 1.7585e-04 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.7370e-04 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 1.7170e-04 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 158ms/step - loss: 1.6972e-04 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 1.6782e-04 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 1.6595e-04 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 176ms/step - loss: 1.6417e-04 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 1.6242e-04 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 1.6066e-04 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 1.5897e-04 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 1.5730e-04 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.5570e-04 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 1.5411e-04 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 1.5250e-04 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.5098e-04 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 1.4946e-04 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.4800e-04 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 1.4651e-04 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 1.4516e-04 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 1.4373e-04 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.4232e-04 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.4097e-04 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.3963e-04 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 1.3836e-04 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 1.3710e-04 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 1.3589e-04 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 1.3461e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.3340e-04 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 1.3221e-04 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.3108e-04 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 1.2991e-04 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.2880e-04 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.2769e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.2661e-04 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.2553e-04 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.2446e-04 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 1.2345e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b019cdac0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "TteS29ErXKNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sen = preprocess('ما هي المنصة')\n",
        "train_X_seq = tokenizer.texts_to_sequences([sen])\n",
        "train_X_pad = pad_sequences(train_X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(train_X_pad)\n",
        "result = model.predict(train_X_pad)\n",
        "print(result)\n",
        "print(cat_encoder.categories_[0][result.argmax()])\n",
        "print(result.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaDNjGDsu6p3",
        "outputId": "8380f85a-a39f-4af2-a1b4-526ae2724d6b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "[[1.2645840e-15 4.7119960e-12 1.9210051e-09 5.1304269e-12 1.9900085e-08\n",
            "  6.0202225e-11 5.2022700e-05 9.9988377e-01 2.3839291e-07 6.3859712e-05\n",
            "  5.8871339e-09]]\n",
            "هي بوابة رقمية تحتوى علي العديد من الخدمات الالكترونية\n",
            "0.9998838\n"
          ]
        }
      ]
    }
  ]
}