{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzUPlpWmbfBx",
        "outputId": "462c7758-cc3f-407c-d3b5-fe5e9bb0b072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk import word_tokenize\n",
        "from string import punctuation\n",
        "\n",
        "# Load FAQs\n",
        "Questions_PATH=\"./\"\n",
        "\n",
        "def load_dataset(dataset_path=Questions_PATH):\n",
        "    csv_path = os.path.join(dataset_path, \"faqs.csv\")\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "data_original = load_dataset()\n",
        "data = load_dataset()\n",
        "\n",
        "# Define stemmer\n",
        "stemmer = ISRIStemmer()\n",
        "\n",
        "# Define stopwords\n",
        "stopwords = set(nltk.corpus.stopwords.words('arabic') + list(punctuation))\n",
        "\n",
        "# Prepare data with stemming and remove stopwords\n",
        "def preprocess(text):\n",
        "    words = word_tokenize(str(text))\n",
        "    words = [word for word in words if word not in stopwords]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    data['question'][index] = preprocess(row['question'])\n",
        "    data['answer'][index] = preprocess(row['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit tokenizer on data\n",
        "tokenizer.fit_on_texts(data)\n",
        "\n",
        "# Convert data to sequences\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "# Pad sequences\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "max_len = max([len(seq) for seq in sequences])\n",
        "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Define model architecture\n",
        "out_dim = 64\n",
        "lstm_dim = 64\n",
        "dense_dim = 64\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=out_dim, input_length=max_len),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_dim)),\n",
        "    tf.keras.layers.Dense(dense_dim, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "opt = Adamax(learning_rate=0.03, beta_1=0.8, beta_2=0.9999)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 40\n",
        "\n",
        "# Train model\n",
        "model.fit(padded_sequences, padded_sequences, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgwkLutolrmw",
        "outputId": "1803a7ba-7d0e-4cf2-8eb7-214cdbedf230"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_69\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_69 (Embedding)    (None, 1, 64)             192       \n",
            "                                                                 \n",
            " bidirectional_69 (Bidirecti  (None, 128)              66048     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_138 (Dense)           (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_139 (Dense)           (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 74,561\n",
            "Trainable params: 74,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 2/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 3/40\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 4/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 5/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 6/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 7/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 8/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 9/40\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 10/40\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 11/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 12/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 13/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 14/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 15/40\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 16/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 17/40\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 18/40\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 19/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 20/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 21/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 22/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 23/40\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 24/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 25/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 26/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 27/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 28/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 29/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 30/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 31/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 32/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 33/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 34/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 35/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 36/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 37/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 38/40\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 39/40\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n",
            "Epoch 40/40\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae2b2b1b80>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen = preprocess('يعني ايه اعلانات')\n",
        "train_X_seq = tokenizer.texts_to_sequences([sen])\n",
        "train_X_pad = tf.keras.preprocessing.sequence.pad_sequences(train_X_seq, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "result = model.predict(train_X_pad)\n",
        "print(result)\n",
        "print(data_original.answer[result.argmax()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaDNjGDsu6p3",
        "outputId": "e25001a2-24c2-4b8e-c4e5-5d758ae17373"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[1.]]\n",
            "هي بوابة رقمية تحتوى علي العديد من الخدمات الالكترونية\n"
          ]
        }
      ]
    }
  ]
}